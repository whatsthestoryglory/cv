---
title: "Unsupervised Learning Methods"
author: "Cameron"
date: 2021-03-11T11:04:25-05:00
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Unsupervised Statistical Learning Methods

Earlier this week I completed a Statistical Learning course from Stanford put on by Trevor Hastie and Rob Tibshirani.  I was drawn to this course because I had stumbled upon `An Introduction to Statistical Learning` as I looked to learn more about how to measure, and perhaps more importantly, visualize the Quality data I was reviewing in my role as a Quality Manager.  The following post is more or less the same as the .rmd created within the course as I wrote it while following along with their video.  I'm posting it here as an exercise both in learning `blogdown` to build this site, but also to drive me to make further improvements.  You may notice some notes I left for myself that I wanted to come back to.  Without further adeiu, I bring you Principal Components Analysis.


## Principal Components Analysis

Start by using the `USArrests` data which comes built in to `R`

```{r load_data}
dimnames(USArrests)
apply(USArrests,2,mean)
apply(USArrests,2,var)
```

From the above output you can see that `Assault` has significantly higher variance than the other variables and would dominate the principal components so we're going to standardize the variables before performing PCA

### Standardization

```{r standardize}
pca.out=prcomp(USArrests, scale=TRUE)
pca.out
names(pca.out)
biplot(pca.out, scale=0, cex=.6)
library(ggfortify)
autoplot(pca.out, loadings=TRUE, label=TRUE, label.size = 3, loadings.label = TRUE, shape=FALSE)
```

*Can I change biplot to something in plotly or ggplot?  Come back and check...*

Here we see the directions of the loadings of each variable against the principal components chosen in red, and each state showing where it lands in the two principal components

Recall how in the previous chunk the loading vectors were negative, so they pull to the left of the graphic.

Note how 3 of the 4 variables are generally towards the same direction of the X axis, Murder, Assault, and Rape.  So we can say from this that PC1 is a general measure of overall crime.

Then looking at PC2, the Y axis, we can see that it's mostly a measure of urban population.

## K-Means Clustering

K-Means clustering can be applied to any dimension but is going to be demonstrated here in two because it makes plotting pictures easy

`` I should consider coming back to this exercise to do it in 3 dimensions and make a 3D plotly plot of it ``

``` {r}
set.seed(101)
x=matrix(rnorm(100*2),100,2)
xmean=matrix(rnorm(8,sd=4),4,2)
which=sample(1:4,100,replace=TRUE)
x=x+xmean[which,]
plot(x,col=which,pch=19)
```

So with this data we know the clusters but now we're going to strip that data and provide it to the k-means clustering algorithm and see how it does

``` {r}
km.out=kmeans(x,4,nstart=15)
km.out
plot(x,col=km.out$cluster,cex=2,pch=1,lwd=2)
points(x,col=which,pch=19)
points(x, col=c(1,2,3,4)[which],pch=19)
```

As you can see the colors of the rings (known) are almost exactly matching the color of the points (calculated using `kmeans`)

## Hierarchical Clustering

Let's see how the same data from above is handled by hierarchical clustering

``` {r}
hc.complete=hclust(dist(x), method="complete")
plot(hc.complete)
```

The plot shows using the complete method, which makes its decisions using the clustered points that are furthest apart.  There are other methods that we could use that produce slightly different outputs

``` {r}
hc.single=hclust(dist(x),method="single")
plot(hc.single)
```

Using `method="single"` we do the opposite of complete, measure by the closest points within each cluster.  You can see that with this it tends to find longer more drawn out clusters rather than the nice large jumps found in the earlier method.


``` {r}
hc.average=hclust(dist(x),method="average")
plot(hc.average)
```

As you can imagine, using `method="average"` we use the average distance between each cluster to measure and you can see the resulting plot is somewhere in between the first two.

-------------------------------------------------

Now let's try comparing our results with the actual clusters in the data we built.

We're going to use the `cutree` function to cut the tree at level 4.  This will produce a vector of numbers from 1 to 4 saying which branch each observation is on.  We're going to use the complete cluster to cut from.

``` {r}
hc.cut=cutree(hc.complete,4)
table(hc.cut, which)
table(hc.cut, km.out$cluster)
```

With the above outputs we can see that the hierarchical tree cut at 4 ended up with very similar results to the kmeans cluster from before and to the original output.

``` {r}
plot(hc.complete, labels=which)

```

*I'm also going to come back to this and redo it so the labels are color coded instead of number coded, it looks awful...*

And above we can see how our results compare to the original choices.  It's not particularly clear doing it this way but you can spot the mismatches relatively easily.